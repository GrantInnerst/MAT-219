\documentclass{phd}\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

\voffset=-.6875in
\setlength{\textheight}{9.65in}

%% packages
\usepackage{amsfonts, amssymb, amsmath, amsthm, latexsym}
\usepackage{natbib, graphicx, rotating, float}
\usepackage{listings,booktabs}
\usepackage{url}
\usepackage{bm}
\usepackage{subfig}
\usepackage{gensymb}
\usepackage{multirow}
\usepackage{array}
\usepackage[linesnumbered,boxed]{algorithm2e}
\usepackage{dcolumn} 
\usepackage{framed}
\newcolumntype{d}[1]{D{.}{\cdot}{#1} }

%% input the macros
% \input{__0-macros.tex}

%% do figs pathing -- make sure you have a folder named 'figures' in the directory
%\graphicspath{{figures/}}  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Front matter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Methods in Algebraic Statistics}
\author{Grant Innerst}
\degrees{B.S., M.S.}  % degrees held before this Ph.D.

% your committee members and degree in alphabetical order
% I believe external members go last regardless of alphabetical ordering
\mentor{David J. Kahle, Ph.D.}
\readerThree{Jane Harvill, Ph.D.}
\readerFour{James D. Stamey, Ph.D.}    
\readerFive{Dean M. Young, Ph.D.}
\readerSix{John Tripp, Ph.D.}  

\confDate{May 2019}   % month and year of actual graduation

\makeCopyrightPage

\graduateDean{J. Larry Lyon, Ph.D.}
\deptChair{Jack D. Tubbs, Ph.D.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ABSTRACT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\abstract{Abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DEDICATION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\dedication{Dedication}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ACKNOWLEDGEMENTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\acknowledgements{None so far}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DOCUMENT BODY
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}




This document is meant to be used as a writing board that I will eventually use to complete the chapters of my dissertation. 


\section{Contributions to the Application of Markov Bases Strategies}
\subsection{Hit-and-Run Algorithm}
The current MCMC algorithm proposed by Diaconis and Sturmfels (1998) suffers from poor mixing behavior and efficiency problems that are common to MCMC algorithms. One idea, proposed by Diaconis and Sturmfels in their original paper, is to propose some multiple of the original move. Proposing such a multiple of the original move that still results in a table inside the fiber may improve the algorithms behavior by exploring the fiber faster and making the Markov Chain random much more quickly. When the fiber is large, as they often are in practice (example of the Hair and Eye color dataset of Birth-Death dataset) the original algorithm may take a comparatively long time(in the number of steps) to reach the steady-state distribution of interest (here the hypergeometric distribution on the fiber), while using multiples of those moves will aid in the exploration of the fiber more quickly. We will give a proper algorithmic context to this proposed algorithm enhancement. This proposed algorithm is loosely related to the discrete hit and run algorithm and will be referred to as such in this work. The steps of the discrete hit and run algorithm for choosing a move are as follows:
\begin{enumerate}
\item Choose a move uniformly from the move set.
\item Compute the integer bounds of the fiber with respect to the chosen move.
\item Choose a value from the range found in (2) uniformly.
\item Apply the move a specified number of times.
\end{enumerate}
The essence of the discrete hit and run algorithm is choosing the integer bounds on how many moves can be made without venturing outside the fiber. A table that is inside the fiber has the properties that the marginal totals will be the same as the observed table, and none of the cell entries are negative. The first property is always met due to the nature of the Markov basis and MCMC algorithm. Specifically, the moves have zero row and column totals and so applying them to the current table will result in a table with the same marginal tables. The second property, that all of the cell entries are non-negative, is not guaranteed by the properties of moves or the algorithm. Thus, in computing the integer bounds of the fiber with respect to the chosen move, we are checking how many times that specific move can be applied without a cell entry becoming negative. Thus, at least theoretically, solving for these integer bounds will only involve solving independent linear equations and then systematically choosing between those set of solutions. Specifically, let \textbf{t} be the current table in vectorized form, \textbf{m} be the selected move in vectorized form, \textbf{p} be the vectorized proposed move, and \textbf{c} be a vector that represents the number of times the selected move can be applied until the cell becomes 0. The original proposed move is calculated as $$\textbf{t} + \textbf{m} = \textbf{p}$$ while our algorithm would calculate the proposal as $$\textbf{t} + c * \textbf{m} = \textbf{p}$$ where $c$ is the integer constant chosen from our proposed algorithm. Taking the proposal calculation above, the set of linear equations we are solving for can be expressed as $$\textbf{t} + \textbf{c} * \textbf{m} = \mathbf{0}$$ or in terms of \textbf{c} as $$\textbf{c} = -\frac{\textbf{t}}{\textbf{m}}$$ where the division is done element-wise. After the resulting division has taken place, the bounds need to be chosen from the set of integers in \textbf{c}. The bounds are chosen to be the minimum of the positive values and the maximum of the negative values in \textbf{c}, $(\{ \max ~\textbf{c} ~|~ \textbf{c} < 0\} , \{ \min ~\textbf{c} ~|~ \textbf{c} > 0\})$. These values represent the boundaries of the fiber in some dimension with respect to the move. In terms of entries in the table, these bounds represent the number of times a move can be applied until one or more entries are 0. 

There is a special case in which our proposed algorithm does not always guarantee a proposal inside the fiber in the current form. This special case occurs when the current table is on the boundary of the fiber (i.e. when one or more of the cell entries is 0). In the integer bounds calculation, the resulting vector \textbf{c} will contain one or more zeros. In choosing the bounds to be $[\{ \max ~\textbf{c} ~|~ \textbf{c} < 0\} , \{ \min ~\textbf{c} ~|~ \textbf{c} > 0\}]$, any 0 elements are neglected and therefore the bounds may contain multiples of the move that will result in a table outside of the fiber. For example, let 
$$\textbf{t} = \begin{bmatrix}
7 & 4 & 9 & 0
\end{bmatrix}, ~~~\text{and}~~~ \textbf{m} = \begin{bmatrix}1 & -1 & -1 & 1 \end{bmatrix}.$$

The resulting division would yield $\textbf{c} = \begin{bmatrix} -7 & 4 & 9 & 0 \end{bmatrix}$ and so the integer bounds would be $[-7, 4]$ but any application of the move in a negative direction (a negative constant $c$) will result in a table outside of the fiber. Thus, to account for this special boundary condition, the algorithm checks each bound to make sure the resulting proposal is valid. If the bound is not valid, the current table is on the boundary and the algorithm needs to reset the bounds accordingly. If the boundary problem occurs in the negative direction, the lower bound will become 1 and if the boundary problem occurs in the positive direction, the upper bound will become -1. If both bounds are not valid, the range will become [-1,1], which mimics the behavior of proposal selection in the overall algorithm. (When both bounds have to reset, that is a clear indication that any proposed move will result in a table outside of the fiber, but it is good form to reset them in an algorithmic sense?) It is important to note that 0 is not included in any of the ranges because proposing the current table is detrimental to the overall algorithm's behavior. An implementation of this addition to the Metropolis-Hastings algorithm is available in the \verb|R| package \verb|algstat| through a formal argument option \verb|hitAndRun| in the function \verb|metropolis|. This implementation is coded in C++ through the \verb|R| package \verb|Rcpp| to optimize speed and to mesh with the original algorithm, which was also coded through \verb|Rcpp|. 

As an example of how the algorithm works in the implementation and how it compares to the base proposal algorithm, consider Snee's hair and eye color data set (1974). This data set comes from a survey of college students at the University of Delaware and contains 592 observations. The original data set in the software package \verb|R| contains three variables (hair, eye, and sex) but we will aggregate over sex to agree with the table in Snee (1974). The data set is given below:
\vskip .1in

\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Brown & Blue & Hazel & Green \\ 
  \hline
Black & 68 & 20 & 15 & 5 \\ 
  Brown & 119 & 84 & 54 & 29 \\ 
  Red & 26 & 17 & 14 & 14 \\ 
  Blonde & 7 & 94 & 10 & 16 \\ 
   \hline
\end{tabular}
\end{table}

Notice that this data set has large cell counts (Non-sparse) and so the $\chi^2$ tests, which require large samples for the convergence of the distribution of the test statistic, should work very well. This data set is used because it should be well behaved and helps to illustrate some points made in this paper. Also, the distribution of our statistic can be compared to the theoretical $\chi^2$ distribution (the asymptotic approximation). 

To illustrate how the hit and run like algorithm works compared to the base scheme, one Markov chain of length 1000 with 0 burn-in iterations and no thinning are produced for each method with the independence model (one of the simplest model structures). Below is a trace plot of the two chains:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in file(con, "{}r"{}): cannot open the connection}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in metropolis(tbl, moves, suffStats = suff\_stats, config = A): unused arguments (suffStats = suff\_stats, config = A)}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in metropolis(tbl, moves, suffStats = suff\_stats, config = A, hitAndRun = TRUE): unused arguments (suffStats = suff\_stats, config = A, hitAndRun = TRUE)}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in algstat:::computeUProbsCpp(base\$steps): object 'base' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in algstat:::computeUProbsCpp(har\$steps): object 'har' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in eval\_tidy(xs[[i]], unique\_output): object 'base\_steps' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in as.ts(x): object 'base\_steps' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in as.ts(x): object 'har\_steps' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in as.vector(base\_acf\$acf): object 'base\_acf' not found}}\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in ggplot(data = plot\_df): object 'plot\_df' not found}}\end{kframe}
\end{knitrout}

One proposed improvement of the hit and run scheme is efficiency, which is defined as how quickly the Markov Chain converges to the stationary distribution (in this case the hypergeometric distribution on the fiber). In this particular instance, the Markov chain that was created by the algorithm that uses the hit and run proposal scheme seems to converge at iteration 200 while the Markov chain created by the algorithm using the default proposal scheme levels converge at iteration 500. Thus, it seems the proposed scheme seems increase the efficiency of the overall algorithm, although one chain is not enough evidence to use as a basis for conclusion. 

The hypergeometric distribution on the fiber is the main distribution of interest in our models but another that can be used to illustrate the improved efficiency is the uniform distribution on the fiber. Again, one chain from each method will be run for 1000 iterations with no thinning or burn-in samples. Below are time-series plots of each method:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in metropolis(tbl, moves, suffStats = suff\_stats, config = A, dist = "{}uniform"{}): unused arguments (suffStats = suff\_stats, config = A)}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in metropolis(tbl, moves, suffStats = suff\_stats, config = A, hitAndRun = TRUE, : unused arguments (suffStats = suff\_stats, config = A, hitAndRun = TRUE)}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in algstat:::computeUProbsCpp(base\_unif\$steps): object 'base\_unif' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in algstat:::computeUProbsCpp(har\_unif\$steps): object 'har\_unif' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in eval\_tidy(xs[[i]], unique\_output): object 'base\_unif\_steps' not found}}\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in ggplot(plot\_unif\_df): object 'plot\_unif\_df' not found}}\end{kframe}
\end{knitrout}

It is clear, from the above plot, that the hit and run proposal scheme outperforms the base proposal scheme by a wide margin. The chain produced by the hit and run proposal scheme explores a much larger portion of the fiber than the default scheme. It is not surprising that this behavior occurs because the metropolis algorithm to generate a uniform distribution on the fiber will accept any proposal unless it falls outside of the fiber. The "large" proposal moves that the hit and run scheme produces, enables the Metropolis-Hastings algorithm to explore the fiber much more quickly.

Another proposed improvement, originally mentioned in the original work by Diaconis and Sturmfels (1998), was that the resulting chain will become random much more quickly. Autocorrelation function plots can be used to assess the mixing behavior of chain(s). Chains that have large autocorrelation function values take a long time to traverse the sample space, in this specific case the fiber, which is not a desirable property. Consider the plot of the autocorrelations for each chain shown in the first plot:


\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in ggplot(data = plot\_auto\_df): object 'plot\_auto\_df' not found}}\end{kframe}
\end{knitrout}

We see that the autocorrelation for our proposed algorithm decreases much more quickly than the original algorithm scheme, although both autocorrelation plots do not exhibit satisfactory dampening behavior and thinning would be advisable. The quicker decrease of the proposed hit and run like algorithm is most-likely due to its ability to propose "larger" moves than the original proposal scheme. 

Again, it is important to note that the behavior of one chain is not enough evidence to use as a basis for conclusion as to which method performs better. To address this, 100 chains for each method will be created. Each chain will be run for 1000 iterations with no thinning and no burn-in samples. Again, the independence model will be used and so the distribution of interest is the hypergeometric distribution on the fiber. Below is a time series plot of the chains for each method:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in metropolis(tbl, moves, suffStats = suff\_stats, config = A): unused arguments (suffStats = suff\_stats, config = A)}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in metropolis(tbl, moves, suffStats = suff\_stats, config = A, hitAndRun = TRUE): unused arguments (suffStats = suff\_stats, config = A, hitAndRun = TRUE)}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in eval(lhs, parent, parent): object 'base\_chains' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in eval(lhs, parent, parent): object 'har\_chains' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in dots\_values(...): object 'base\_df' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in plot\_df\$iter <- 1:1000: object 'plot\_df' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in eval(lhs, parent, parent): object 'plot\_df' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in plot\_df\$group\_id <- c(rep("{}base"{}, 100 * 1000), rep("{}har"{}, 100 * : object 'plot\_df' not found}}\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in eval(lhs, parent, parent): object 'plot\_df' not found}}\end{kframe}
\end{knitrout}

From the above plot, it is clear to see that the hit and run proposal scheme outperforms the base proposal scheme by a wide margin in this specific situation. Nearly all of the chains produced by using the hit and run scheme can claim convergence by iteration 250 while the chains produced by the base proposal scheme do not start to converge until iteration 600. 

One last consideration we need to address is speed. If this new proposal scheme increases the computational time substantially, the improved behavior addressed above becomes a moot point because thinning can be employed to improve the results of the original algorithm. To measure the comparative computational speed of the new scheme, we can benchmark the algorithms using the \verb|R| package \verb|microbenchmark|. The \verb|microbenchmark| function will evaluate an expression for a specified number of times and return a summary of the evaluation times is returned for each expression. We are going to use the same problem setup as in previous examples. The base scheme will be evaluate first and the hit and run scheme second. The results for each chain follow:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in metropolis(tbl, moves, suffStats = suff\_stats, config = A): unused arguments (suffStats = suff\_stats, config = A)}}\end{kframe}
\end{knitrout}

From these results, we see the hit and run scheme is around twice as slow as the base scheme. While slightly more computationally intensive, the hit and run scheme is still very fast. The difference in time of execution between the two is indistinguishable to the naked eye.  

In order to truly assess how this scheme works, other datasets need to be examined. Below is code that generates a $10 \times 10$ two way contingency table with very small cell entries. I have not elaborated on this section but I have made the necessary fixes so that the hit and run scheme does not error. 

\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{test} \hlkwb{<-} \hlkwd{sample}\hlstd{(}\hlnum{0}\hlopt{:}\hlnum{2}\hlstd{,} \hlnum{100}\hlstd{,} \hlkwc{replace} \hlstd{= T)} \hlopt{%>%} \hlkwd{matrix}\hlstd{(}\hlnum{10}\hlstd{,} \hlnum{10}\hlstd{)}

\hlstd{vec} \hlkwb{<-} \hlkwd{tab2vec}\hlstd{(test)}
\hlstd{tmp_A} \hlkwb{<-} \hlkwd{hmat}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{10}\hlstd{,} \hlnum{10}\hlstd{),} \hlnum{1}\hlopt{:}\hlnum{2}\hlstd{)}
\hlstd{tmp_suff_stats} \hlkwb{<-} \hlstd{tmp_A} \hlopt{%*%} \hlstd{vec}
\hlstd{tmp_moves} \hlkwb{<-} \hlkwd{markov}\hlstd{(tmp_A)}

\hlstd{expected} \hlkwb{<-} \hlkwd{chisq.test}\hlstd{(vec)}\hlopt{$}\hlstd{expected}

\hlstd{tmp_base} \hlkwb{<-} \hlkwd{metropolis}\hlstd{(vec, tmp_moves,} \hlkwc{suffStats} \hlstd{= tmp_suff_stats,}
    \hlkwc{config} \hlstd{= tmp_A,} \hlkwc{iter} \hlstd{=} \hlnum{5000}\hlstd{)}
\hlstd{tmp_har} \hlkwb{<-} \hlkwd{metropolis}\hlstd{(vec, tmp_moves,} \hlkwc{suffStats} \hlstd{= tmp_suff_stats,}
    \hlkwc{config} \hlstd{= tmp_A,} \hlkwc{hitAndRun} \hlstd{= T,} \hlkwc{iter} \hlstd{=} \hlnum{5000}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}


One troubling observation from the time-series plot of the single "hit and run" chain could be described as blocky behavior in some sections. This behavior corresponds to rejecting proposed moves many times in succession. The continual rejection of moves will increase the autocorrelation function values and in general is not a desirable property of the algorithm. This behavior also indicates that when tables become sparser, the acceptance rate will become worse, even to the point where the hit and run scheme may not be able to propose any table inside the fiber. An acceptance rate of the chain is provided in the output of the \verb|R| function \verb|metropolis|. To illustrate this point, 100 chains are generated from each scheme, and a summary of the respective acceptance rates is given below:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in metropolis(tbl, moves, suffStats = suff\_stats, config = A): unused arguments (suffStats = suff\_stats, config = A)}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in metropolis(tbl, moves, suffStats = suff\_stats, config = A, hitAndRun = TRUE): unused arguments (suffStats = suff\_stats, config = A, hitAndRun = TRUE)}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in summary(base\_acceptprob): object 'base\_acceptprob' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in summary(har\_acceptprob): object 'har\_acceptprob' not found}}\end{kframe}
\end{knitrout}
\subsection{Adaptive Schemes}
In order to address the problem of a very low acceptance rate, we propose an alternate method that leverages the hit and run's ability to quickly explore the fiber while addressing the behavior that led to low acceptance rates. After computing the integer bounds on the fiber with respect to the chosen move, the hit and run scheme selects uniformly from those bounds in order to create a proposal, which is not optimal in many situations. In choosing from the bounds uniformly, we allow for the creation of a proposal that may severely overshoot or undershoot the distribution of interest because we are not taking into account any information about said distribution. An attractive feature of the hit and run scheme is its ability to propose large moves which can move around the fiber more quickly. Unfortunately, the scheme does so in a "mindless" way that leads to the unattractive behavior of low acceptance rates. For instance, when a chain has begun to converge to the distribution of interest, any large move that creates a proposal outside the distribution's mass will be rejected. In choosing from the bounds uniformly, each possible multiple, many of whom would move away from the distribution's mass, is equally likely to be chosen.

In order to incorporate information about the distribution of interest, we propose an adaptive scheme that uses an MCMC algorithm at each step of the outer MCMC algorithm. At every iteration of the main MCMC algorithm, we will calculate the bounds, as in the hit and run scheme, and then run another Metropolis-Hastings algorithm with the base proposal scheme to calculate the proposal. The proposal distribution of the inner MCMC becomes the selected move from the outer MCMC. The number of iterations is chosen to be the number of distinct multiples available, found by subtracting the lower bound from the upper bound. This Metropolis-Hastings algorithm will still hypothetically converge to the hypergeometric distribution on the fiber and thus we are incorporating information about the distribution. The inner chain is run for such a short number of iterations because of computational complexity considerations. This scheme will allieviate overshooting or undershooting the distribution's mass because the inner Markov chain will never venture into low probability regions. 

In theory, this scheme will allow the algorithm to still converge to the distribution of interest at least as quickly as the hit and run scheme while have an acceptance rate as good as the base scheme. Again, we will explore the scheme's behavior with the hair and eye color dataset, this time running chains for 5000 iterations with no burnin samples or thinning with the independence model. Below is a time-series plot of one chain for each method.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in metropolis(tbl, moves, suff\_stats, A, hitAndRun = T, adaptive = T): unused argument (hitAndRun = T)}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in eval(lhs, parent, parent): object 'adapt' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in eval\_tidy(xs[[i]], unique\_output): object 'base\_steps' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in eval(lhs, parent, parent): object 'plot\_adapt' not found}}\end{kframe}
\end{knitrout}

Looking at the above plot, we see the adaptive scheme does have the desired behavior outlined above. The adaptive scheme's chain reaches the distirbution's mass first and does not have blocky behavior associated with low acceptance rates. Indeed, the adaptive scheme has acceptance rates comparable to the base scheme. The adaptive scheme also boasts lower autocorrelation function values than both previous schemes. This is not surprising because it inherits large move capability while losing low acceptance rates. Below is a plot comparing the sample autocorrelation functions for each method. 

\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in as.ts(x): object 'adapt\_steps' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in as.vector(base\_acf\$acf): object 'base\_acf' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in eval(lhs, parent, parent): object 'plot\_auto\_df2' not found}}\end{kframe}
\end{knitrout}

Again, one chain is not enough information to use as a basis for conclusion because of the randomness involved in the process. We will generate an additional 100 chains from the adaptive scheme to compare its behavior to the hit and run scheme. Similarly, each chain will be run for 1000 iterations with no burn-in samples and no thinning. The time series plot below shows this comparison. 

\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in metropolis(tbl, moves, suffStats = suff\_stats, config = A, hitAndRun = T, : unused arguments (suffStats = suff\_stats, config = A, hitAndRun = T)}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in eval(lhs, parent, parent): object 'adapt\_chains' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in dots\_values(...): object 'har\_df' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in plot\_df2\$iter <- 1:1000: object 'plot\_df2' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in eval(lhs, parent, parent): object 'plot\_df2' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in plot\_df2\$group\_id <- c(rep("{}har"{}, 100 * 1000), rep("{}adapt"{}, 100 * : object 'plot\_df2' not found}}\end{kframe}
\end{knitrout}


\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in eval(lhs, parent, parent): object 'plot\_df2' not found}}\end{kframe}
\end{knitrout}

This plot seems to corroborate the observations above that the adaptive scheme reaches the mass of the distribution faster and has much higher accptance rates than the hit-and-run scheme. One troubling observation from this plot is that while the chains from both methods are mixing, the chains from the adaptive scheme seem to not explore the mass of the distribution as much as the hit and run scheme chains. In order to further assess if the chains are really converging to the hypergergeomtric distribution on the fiber, we are going to generate a chain of length 50000, calculate Pearson $\chi^2$ values for each table in the chain, and plot their density with the appropriate $\chi^2$ distribution overlayed. For tables with larger cell totals, the $\chi^2$ distribution is a good approximation and so the density of $\chi^2$ values should match up with the appropiate $\chi^2$ distribution. For the hair and eye color data set, the appropriate $\chi^2$ distribution is a $\chi^2$ with 9 degrees of freedom. Below is the density plot with the $\chi^2_9$ distribution overlayed in red.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in metropolis(tbl, moves, suff\_stats, A, hitAndRun = T, adaptive = T, : unused argument (hitAndRun = T)}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in algstat:::computeX2sCpp(adapt\_chi, exp = expected): object 'adapt\_chi' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in ggplot(adapt\_stats): object 'adapt\_stats' not found}}\end{kframe}
\end{knitrout}

From this plot, we see that the density of $\chi^2$ values from the chain does not seem to be converging to the correct $\chi^2$ distribution. Other avenues, such as lengthening the chain, significantly thinning, and burn-in samples have been used in order to assess if the chain does converge, just more slowly, but to no avail. It seems that the inner-MCMC algorithm to choose the proposal for the outer MCMC is not proposing tables throughout the distribution's mass. Looking at the trace plot above, we see that it is rare to see an adaptive chain dip below an un-normalized log-likelihood value of -1750 or even -1747 while chains from the hit and run scheme routinely range from -1745 to -1755. It seems as if the adaptive scheme is finding the "mode" with repect to the hypergeomtric distribution on the fiber but not moving around the entire mass. One possible idea was to change the length of the inner chain. Multiple line length were tried but results only got worse and/or more computationally intensive. 

Another downside noticed in this exploration was that this scheme increases the computational time exponentially. For every iteration of the outside algorithm, there are a possibly large number of steps added to the computational time. Even though each single iteration does not take much time, the sheer number of total iterations taken between the two algorithms makes this scheme computationally intensive. 

\textbf{I am currently thinking about other possible fixes to this problem!}


\subsection{Combination Scheme(s)}

The hit and run scheme is incredibly useful, especially earlier in the chain, because it has the ability to traverse the fiber quickly, in terms of the number of steps taken, and has better autocorrelation behavior in comparison to the base proposal scheme. Some downfalls are that the hit and run scheme has low acceptance probabilities, especially once the chain has begun to converge, and could possibly not be able to propose any moves when used on sparse tables. The base scheme has much higher acceptance probabilities and is more likely to be able to move around obscure fibers but is relatively slow to reach the distibution's mass and has suboptimal autocorrelation behavior. A new proposed scheme uses the hit and run proposal scheme for a short number of iterations, say 1/10 of the total number iterations, and then falls back to the base scheme for the rest of the chain. This method should have improved acceptance probability and autocorrelation behavior while still garunteeing that the chain is converging to the correct distribution and is computationally fast. Below is an example of one chain of length 1000 from the new proposed scheme.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in metropolis(tbl, moves, suff\_stats, A, hitAndRun = T, iter = 100): unused argument (hitAndRun = T)}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in unname(init): object 'har\_part' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in cbind(har\_part\$steps, base\_part\$steps): object 'har\_part' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in eval(lhs, parent, parent): object 'total\_steps' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in unlls\$iter <- 1:1000: object 'unlls' not found}}

{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in eval(lhs, parent, parent): object 'unlls' not found}}\end{kframe}
\end{knitrout}

\end{document}
