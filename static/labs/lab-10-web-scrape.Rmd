---
title: "Lab 10 - Say cheers to web scraping!"
date: "2018-04-12"
output: 
  tufte::tufte_html:
    tufte_variant: "envisioned"
    highlight: pygments
    css: lab.css
link-citations: yes
---

```{r setup, include=FALSE}
library(tidyverse)
library(tufte)
library(knitr)
options(
  htmltools.dir.version = FALSE, # for blogdown
  show.signif.stars = FALSE,     # for regression output
  digits = 2
  )
knitr::opts_chunk$set(eval = TRUE)
```

**Due:** 2018-04-19 at noon

<div style= "float:right;position: relative; margin-left: 20px">
```{r image, echo=FALSE, fig.align="right"}
knitr::include_graphics("img/10-web-scrape/beer.png")
```
</div>

In 2017, the U.S. beer industry shipped (sold) 207.4 million barrels of beer â€“ equivalent to more than 2.9 billion cases of 24-12 ounce servings. In addition, the industry shipped approximately 2 million barrels of cider, equivalent to more than 28.3 million cases. Additionally, the U.S. beer industry sells more than $111.1 billion in beer and malt-based beverages to U.S. consumers each year. (Source: [The U.S. Beer Industry]( https://www.nbwa.org/resources/industry-fast-facts))

Do you know how many breweries are around you? And how much beer they brew? In this lab we scrape and analyze data on US breweries.

We will start with getting data on breweries in North Carolina. Then, you will use a similar approach to get data on breweries in a different state of our choosing.

In order to complete this lab you will need a Chrome browser with the [Selector Gadget extension](http://selectorgadget.com/) installed.

```{marginfigure}
By now you should be familiar with instructions for getting started and setting up your git configuration. If not, you can refer to one of the earlier labs.
```

# Packages

In this lab we will work with the `tidyverse`, `rvest`, and `robotstxt` packages. We can install and load them with the following:

```{r load-packages, message=FALSE}
library(tidyverse) 
library(rvest)
library(robotstxt)
```

# The data

We will scrape brewery information from https://www.ratebeer.com/breweries/. RateBeer.com is an in-depth, consumer-driven source of beer information. We will use the state-level brewery lists on this site to first obtain information on all breweries in a given state. Then, we will dive deeper and obtain additional information on each of the breweries in that state, one-by-one, by automating our code to do so.

Before getting started, let's check that a bot has permissions to access pages on this domain.

```{r paths-allowed, warning=FALSE}
paths_allowed("https://www.ratebeer.com/")
```

## North Carolina

The goal of this exercise is scrape the data from

![](img/10-web-scrape/ncbreweries.png)

and save it as the following data frame.

```{r read-nc-page, echo=FALSE, cache=TRUE}
page <- read_html("https://www.ratebeer.com/breweries/north%20carolina/33/213/")

names <- page %>%
  html_nodes("#brewerTable a:nth-child(1)") %>%
  html_text() %>%
  str_trim()

active_cities <- page %>%
  html_nodes(".filter") %>%
  html_text()

closed_cities <- page %>%
  html_nodes("#brewerTable span") %>%
  html_text()

cities <- c(active_cities, closed_cities)

types <- page %>%
  html_nodes("td.hidden-sm") %>%
  html_text()

counts <- page %>%
  html_nodes(".hidden-sm+ td") %>%
  html_text() %>%
  str_trim() %>%
  as.numeric()

ests <- page %>%
  html_nodes("td:nth-child(5)") %>%
  html_text() %>%
  str_trim() %>%
  as.numeric()

urls <- page %>%
  html_nodes("#brewerTable a:nth-child(1)") %>%
  html_attr("href") %>%
  paste0("https://www.ratebeer.com", .)

ncbreweries <- tibble(
  name = names,
  city = cities,
  type = types,
  beercount = counts,
  est = ests,
  status = c(rep("Active", 226), rep("Closed", 25)),
  url = urls
)
```

```{r echo=FALSE}
library(DT)
datatable(ncbreweries)
```

1. Based on the information on the [North Carolina breweries page](https://www.ratebeer.com/breweries/north%20carolina/33/213/), how many total (active + closed) breweries are there in NC?

2. Let's start with the names of breweries. Use the selector gadget to determine the path to the names of breweries, and create a vector containing 251 elements of type character which are the names of the breweries.

Next we get the cities. The paths for the cities on the active vs. closed page are different, `.filter` vs. `#brewerTable span` respectively.

3. Create two vectors, one called `active_cities` and the other `closed_cities` that contain the cities of the active and closed breweries, respectively. Then, combine these vectos with 

```{r eval=FALSE}
cities <- c(active_cities, closed_cities)
```

4. Scrape brewery type, number of beers brewed at each brewery, and year when brewery first opened ("est."). Save these as `types`, `counts`, and `ests`, respectively.

Now for something new. The last piece of information we want is the URL of the brewery. To do so we use the `html_attr` function, which extracts the `"href"` attribute where the URL appears.

```{r eval=FALSE}
urls <- page %>%
  html_nodes("#brewerTable a:nth-child(1)") %>%
  html_attr("href") %>%
  paste0("https://www.ratebeer.com", .)
```

5. Describe what is happening in the last line of the code above.

6. Create a data frame (`tibble`) called `ncbreweries` with column names shown in the table above.

7. There is at least one error in the data: Edenton Brewing Company appears to have been opened in 1900, but this is not true. Find out when this Brewery was opened, and correct the data.

```{r}
ncbreweries <- ncbreweries %>%
  mutate(
    est = ifelse(name == "Edenton Brewing Company", 2003, est)
  )
```

8. Which city in NC has the most breweries? How many breweries are in Durham, NC? What are they?

```{r}
ncbreweries %>%
  count(city) %>%
  arrange(desc(n))
```

8. Recreate the following visualization, and interpret it.

```{r}
ncbreweries %>%
  count(est, type) %>%
  ggplot(aes(x = est, y = n, color = type)) +
    geom_point() +
    geom_line() +
    labs(x = "Year", y = "Number of brewery openings",
         title = "Number of breweries openening each year",
         subtitle = "By type")
```

## Choose your own state

9. Repeat what you did above (potentially with some modifications) to create a similar data frame (with the same columns) for a different state of your choosing.

10. By using the `map` function, also grab the zip code for each of the breweries from their own pages (at the URL you recorded) and add this as a new column to your data frame.

11. Determine which city in that state has the highest number breweries.

12. Determine which city has the youngest breweries, on average.

Some notes for map stuff to be added into lab:

```{r eval=FALSE}
brewworks217 <- read_html("https://www.ratebeer.com/brewers/217-brew-works/30902/")

zip <- brewworks217 %>%
  html_node("span:nth-child(5)") %>%
  html_text()

getzip <- function(url, node = "span:nth-child(5)"){
  #Sys.sleep(runif(1, min = 0, max = 2))
  read_html(url) %>%
    html_node(node) %>%
    html_text()
}

getzip(url = "https://www.ratebeer.com/brewers/217-brew-works/30902/", 
       node = "span:nth-child(5)")

getzip(url = "https://www.ratebeer.com/brewers/barking-duck-brewing-company/20739/",
       node = "span:nth-child(5)")

zips <- map_chr(ncbreweries$url, getzip)
```

